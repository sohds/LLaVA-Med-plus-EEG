{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e71adb-8e2d-4e83-9bcf-c1fe4d207fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:33:07.151503Z",
     "iopub.status.busy": "2024-03-09T03:33:07.150466Z",
     "iopub.status.idle": "2024-03-09T03:33:07.168669Z",
     "shell.execute_reply": "2024-03-09T03:33:07.167411Z",
     "shell.execute_reply.started": "2024-03-09T03:33:07.151503Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall transformers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f12e791-faa8-408e-94d8-d0c64d6f2668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T04:47:25.471694Z",
     "iopub.status.busy": "2024-03-09T04:47:25.471222Z",
     "iopub.status.idle": "2024-03-09T04:47:25.481168Z",
     "shell.execute_reply": "2024-03-09T04:47:25.479181Z",
     "shell.execute_reply.started": "2024-03-09T04:47:25.471694Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers@cae78c46\n",
    "# 위 두 커맨드라인으로 Hugging Face 버전 맞춰주도록 할 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959ba09-30b6-4424-980c-25404d062450",
   "metadata": {},
   "source": [
    "# <code>LlamaConfig</code> class 작업\n",
    "* transformers 모듈에서는 해당 관련 스크립트를 제공하지 않는다. (깃허브 오픈소스에는 있음)\n",
    "* 그래서 따로 오픈 소스에서 긁어와서 작업하도록 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54adbe7-0d30-48b2-b9f3-74bb70d03aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:33:57.493793Z",
     "iopub.status.busy": "2024-03-09T03:33:57.492795Z",
     "iopub.status.idle": "2024-03-09T03:33:57.501779Z",
     "shell.execute_reply": "2024-03-09T03:33:57.499861Z",
     "shell.execute_reply.started": "2024-03-09T03:33:57.493793Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama import LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fafe83ce-16ca-462c-aba1-a347e3dab0f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:33:57.974597Z",
     "iopub.status.busy": "2024-03-09T03:33:57.973511Z",
     "iopub.status.idle": "2024-03-09T03:33:57.982481Z",
     "shell.execute_reply": "2024-03-09T03:33:57.981193Z",
     "shell.execute_reply.started": "2024-03-09T03:33:57.974597Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This is the configuration class to store the configuration of a [`LlamaModel`]. It is used to instantiate an LLaMA\n",
      "    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
      "    defaults will yield a similar configuration to that of the LLaMA-7B.\n",
      "\n",
      "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
      "    documentation from [`PretrainedConfig`] for more information.\n",
      "    번역: 해당 클래스는 ['LlamaModel']의 Config를 사용하기 위해서 선언된 클래스다.\n",
      "    즉, LLaMA 모델에 대해서 각 버전에 따라 사용할 수 있도록 해당 클래스를 통해 Configuration을 제공한다.\n",
      "    Arugment에 따라 모델 아키텍처가 정의되고, 그에 따라 사용하려는 모델의 아키텍처의 Config를 받을 수 있으며,\n",
      "    기본적으로는 LLaMA-7B 모델을 제공한다.\n",
      "\n",
      "\n",
      "    Args:\n",
      "        vocab_size (`int`, *optional*, defaults to 32000):\n",
      "            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n",
      "            `inputs_ids` passed when calling [`LlamaModel`]\n",
      "        hidden_size (`int`, *optional*, defaults to 4096):\n",
      "            Dimension of the hidden representations.\n",
      "        intermediate_size (`int`, *optional*, defaults to 11008):\n",
      "            Dimension of the MLP representations.\n",
      "        num_hidden_layers (`int`, *optional*, defaults to 32):\n",
      "            Number of hidden layers in the Transformer decoder.\n",
      "        num_attention_heads (`int`, *optional*, defaults to 32):\n",
      "            Number of attention heads for each attention layer in the Transformer decoder.\n",
      "        num_key_value_heads (`int`, *optional*):\n",
      "            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
      "            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
      "            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n",
      "            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n",
      "            by meanpooling all the original heads within that group. For more details checkout [this\n",
      "            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n",
      "            `num_attention_heads`.\n",
      "        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n",
      "            The non-linear activation function (function or string) in the decoder.\n",
      "        max_position_embeddings (`int`, *optional*, defaults to 2048):\n",
      "            The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n",
      "            Llama 2 up to 4096, CodeLlama up to 16384.\n",
      "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
      "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
      "        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n",
      "            The epsilon used by the rms normalization layers.\n",
      "        use_cache (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
      "            relevant if `config.is_decoder=True`.\n",
      "        pad_token_id (`int`, *optional*):\n",
      "            Padding token id.\n",
      "        bos_token_id (`int`, *optional*, defaults to 1):\n",
      "            Beginning of stream token id.\n",
      "        eos_token_id (`int`, *optional*, defaults to 2):\n",
      "            End of stream token id.\n",
      "        pretraining_tp (`int`, *optional*, defaults to 1):\n",
      "            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
      "            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to understand more about it. This value is\n",
      "            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
      "            issue](https://github.com/pytorch/pytorch/issues/76232).\n",
      "        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to tie weight embeddings\n",
      "        rope_theta (`float`, *optional*, defaults to 10000.0):\n",
      "            The base period of the RoPE embeddings.\n",
      "        rope_scaling (`Dict`, *optional*):\n",
      "            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
      "            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
      "            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
      "            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n",
      "            these scaling strategies behave:\n",
      "            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n",
      "            experimental feature, subject to breaking API changes in future versions.\n",
      "        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n",
      "            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n",
      "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
      "            The dropout ratio for the attention probabilities.\n",
      "\n",
      "    ```python\n",
      "    >>> from transformers import LlamaModel, LlamaConfig\n",
      "\n",
      "    >>> # Initializing a LLaMA llama-7b style configuration\n",
      "    >>> configuration = LlamaConfig()\n",
      "\n",
      "    >>> # Initializing a model from the llama-7b style configuration\n",
      "    >>> model = LlamaModel(configuration)\n",
      "\n",
      "    >>> # Accessing the model configuration\n",
      "    >>> configuration = model.config\n",
      "    ```\n"
     ]
    }
   ],
   "source": [
    "print(LlamaConfig.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20517c0b-4889-4156-a8be-ef64cab43a03",
   "metadata": {},
   "source": [
    "# LLaMA 패키지 만들기 (<code>LlamaModel</code> class 작업)\n",
    "* <code>modeling_llama.py</code>를 사용하기 위해 해당 파일 내에서 import하는 패키지를 전부 from transformers로 수정해주는 작업을 진행 중\n",
    "* 깃허브 오픈소스에는 있으나 pip를 통해 다운받은 라이브러리 내에는 없는 부분들을 발견\n",
    "* 이를 <code>transformers_not_downloaded</code>라는 이름의 패키지를 생성하고, 깃허브 오픈소스에서 없는 파일들을 가져와서 쓸 수 있게끔 수정하였음\n",
    "* * *\n",
    "* 또한, 이 작업이 끝난 뒤에 <code>LlamaConfig</code>를 통해서 model을 선언하려고 했으나\n",
    "* <code>LLamaConfig</code>에서 <code>_attn_implementation</code>이라는 attribute가 없다는 오류가 발생\n",
    "* <code>LLamaConfig가 상속받는 <code>PretrainedConfig</code>에도 해당 attribute는 없었기 떄문에\n",
    "* <code>LLamaConfig</code>에서 <code>_attn_implementation</code>이라는 attribute 선언 후, default로 <code>LlamaAttetion</code>을 수행하게 함.\n",
    "* * *\n",
    "* 이제 준비는 다 되었으나 메모리가 터져서 RuntimeError 발생..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "398b3595-de28-4e40-b51e-00bb2e2fe762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:09.746321Z",
     "iopub.status.busy": "2024-03-09T03:34:09.745709Z",
     "iopub.status.idle": "2024-03-09T03:34:09.752690Z",
     "shell.execute_reply": "2024-03-09T03:34:09.751693Z",
     "shell.execute_reply.started": "2024-03-09T03:34:09.746321Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06253fb6-88bf-4426-8bfb-b5c636be896f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:10.418194Z",
     "iopub.status.busy": "2024-03-09T03:34:10.417251Z",
     "iopub.status.idle": "2024-03-09T03:34:10.432878Z",
     "shell.execute_reply": "2024-03-09T03:34:10.430882Z",
     "shell.execute_reply.started": "2024-03-09T03:34:10.418194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:\\\\Doby\\\\daiv_llm\\\\model',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\python310.zip',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c053bf2-fa60-42a3-92ff-9cd6bdd62c8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:12.798355Z",
     "iopub.status.busy": "2024-03-09T03:34:12.798355Z",
     "iopub.status.idle": "2024-03-09T03:34:12.812312Z",
     "shell.execute_reply": "2024-03-09T03:34:12.810322Z",
     "shell.execute_reply.started": "2024-03-09T03:34:12.798355Z"
    }
   },
   "outputs": [],
   "source": [
    "pck_path = os.path.abspath('.') + '\\\\transformers_not_downloaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78d61e55-235c-41aa-b35c-47c845fe9e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:13.827598Z",
     "iopub.status.busy": "2024-03-09T03:34:13.826602Z",
     "iopub.status.idle": "2024-03-09T03:34:13.849540Z",
     "shell.execute_reply": "2024-03-09T03:34:13.847546Z",
     "shell.execute_reply.started": "2024-03-09T03:34:13.827598Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(pck_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae0d201b-17b6-472d-abda-c203d6d8315c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:14.469967Z",
     "iopub.status.busy": "2024-03-09T03:34:14.468956Z",
     "iopub.status.idle": "2024-03-09T03:34:14.492886Z",
     "shell.execute_reply": "2024-03-09T03:34:14.491982Z",
     "shell.execute_reply.started": "2024-03-09T03:34:14.469967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:\\\\Doby\\\\daiv_llm\\\\model',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\python310.zip',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'F:\\\\Doby\\\\daiv_llm\\\\model\\\\transformers_not_downloaded']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6043580e-82f8-4f7b-83b2-0e4cc8c27baa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:25.252116Z",
     "iopub.status.busy": "2024-03-09T03:34:25.251124Z",
     "iopub.status.idle": "2024-03-09T03:34:25.271072Z",
     "shell.execute_reply": "2024-03-09T03:34:25.270071Z",
     "shell.execute_reply.started": "2024-03-09T03:34:25.252116Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c99d5587-db96-4c8a-8c56-b9b1130e57c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:28.463489Z",
     "iopub.status.busy": "2024-03-09T03:34:28.463489Z",
     "iopub.status.idle": "2024-03-09T03:34:29.261358Z",
     "shell.execute_reply": "2024-03-09T03:34:29.259362Z",
     "shell.execute_reply.started": "2024-03-09T03:34:28.463489Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.cache_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache, StaticCache\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.cache_utils'"
     ]
    }
   ],
   "source": [
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b432217-b3f0-469e-a7e5-3ffa6bb00e80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:29.930568Z",
     "iopub.status.busy": "2024-03-09T03:34:29.929570Z",
     "iopub.status.idle": "2024-03-09T03:34:29.950516Z",
     "shell.execute_reply": "2024-03-09T03:34:29.949517Z",
     "shell.execute_reply.started": "2024-03-09T03:34:29.930568Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers_not_downloaded.cache_utils import Cache, DynamicCache, StaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42356d58-b0dd-4de4-ae8a-0d6aeda1bec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:31.283306Z",
     "iopub.status.busy": "2024-03-09T03:34:31.282163Z",
     "iopub.status.idle": "2024-03-09T03:34:31.318068Z",
     "shell.execute_reply": "2024-03-09T03:34:31.315076Z",
     "shell.execute_reply.started": "2024-03-09T03:34:31.283306Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.modeling_attn_mask_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttentionMaskConverter\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling_attn_mask_utils'"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2114eff-b631-40ab-a8b1-4da15bf0fd04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:33.037584Z",
     "iopub.status.busy": "2024-03-09T03:34:33.037180Z",
     "iopub.status.idle": "2024-03-09T03:34:33.050549Z",
     "shell.execute_reply": "2024-03-09T03:34:33.048555Z",
     "shell.execute_reply.started": "2024-03-09T03:34:33.037584Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers_not_downloaded.modeling_attn_mask_utils import AttentionMaskConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "347201dd-21c1-47ef-a22f-a955ce9ee9d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:33.975376Z",
     "iopub.status.busy": "2024-03-09T03:34:33.974264Z",
     "iopub.status.idle": "2024-03-09T03:34:33.989216Z",
     "shell.execute_reply": "2024-03-09T03:34:33.987213Z",
     "shell.execute_reply.started": "2024-03-09T03:34:33.975376Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "040fb4ba-31bd-48ee-85b2-278911aa72c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:34.323359Z",
     "iopub.status.busy": "2024-03-09T03:34:34.322389Z",
     "iopub.status.idle": "2024-03-09T03:34:34.329361Z",
     "shell.execute_reply": "2024-03-09T03:34:34.328346Z",
     "shell.execute_reply.started": "2024-03-09T03:34:34.323359Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "481df16c-6ce8-4609-8ae1-0308f40005f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:34.828112Z",
     "iopub.status.busy": "2024-03-09T03:34:34.827744Z",
     "iopub.status.idle": "2024-03-09T03:34:34.848688Z",
     "shell.execute_reply": "2024-03-09T03:34:34.847689Z",
     "shell.execute_reply.started": "2024-03-09T03:34:34.828112Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecf5221c-8bc9-4d4d-9341-ef0047fdc6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:35.372299Z",
     "iopub.status.busy": "2024-03-09T03:34:35.371408Z",
     "iopub.status.idle": "2024-03-09T03:34:35.413200Z",
     "shell.execute_reply": "2024-03-09T03:34:35.411210Z",
     "shell.execute_reply.started": "2024-03-09T03:34:35.371408Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_flash_attn_2_available' from 'transformers.utils' (C:\\Users\\user\\anaconda3\\lib\\site-packages\\transformers\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     add_start_docstrings,\n\u001b[0;32m      3\u001b[0m     add_start_docstrings_to_model_forward,\n\u001b[0;32m      4\u001b[0m     is_flash_attn_2_available,\n\u001b[0;32m      5\u001b[0m     is_flash_attn_greater_or_equal_2_10,\n\u001b[0;32m      6\u001b[0m     logging,\n\u001b[0;32m      7\u001b[0m     replace_return_docstrings,\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_flash_attn_2_available' from 'transformers.utils' (C:\\Users\\user\\anaconda3\\lib\\site-packages\\transformers\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db18564c-04f8-4994-85f1-c0b999b385d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:35.845541Z",
     "iopub.status.busy": "2024-03-09T03:34:35.844515Z",
     "iopub.status.idle": "2024-03-09T03:34:35.853489Z",
     "shell.execute_reply": "2024-03-09T03:34:35.851574Z",
     "shell.execute_reply.started": "2024-03-09T03:34:35.845541Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "\n",
    "from transformers_not_downloaded.utils import (\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71852cde-25bd-4d5c-b595-1062974d6707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T04:20:06.728429Z",
     "iopub.status.busy": "2024-03-09T04:20:06.727905Z",
     "iopub.status.idle": "2024-03-09T04:28:25.616825Z",
     "shell.execute_reply": "2024-03-09T04:28:25.612838Z",
     "shell.execute_reply.started": "2024-03-09T04:20:06.728429Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 180355072 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mllama\u001b[39;00m\n\u001b[0;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m llama\u001b[38;5;241m.\u001b[39mLlamaConfig()\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\Doby\\daiv_llm\\model\\llama\\modeling_llama.py:959\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m--> 959\u001b[0m     [LlamaDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[0;32m    960\u001b[0m )\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Doby\\daiv_llm\\model\\llama\\modeling_llama.py:959\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m--> 959\u001b[0m     [\u001b[43mLlamaDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[0;32m    960\u001b[0m )\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Doby\\daiv_llm\\model\\llama\\modeling_llama.py:732\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[1;34m(self, config, layer_idx)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# LLaMA의 Decoder Layer에서 Attention 메커니즘을 어떻게 가져갈 것인가를 고르는 부분\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# 하지만, Configuration 쪽에는 _attn_implementation이 없다.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# LlamaConfig, 이걸 상속해준 PretrainedConfig도 마찬가지\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m LLAMA_ATTENTION_CLASSES[config\u001b[38;5;241m.\u001b[39m_attn_implementation](config\u001b[38;5;241m=\u001b[39mconfig, layer_idx\u001b[38;5;241m=\u001b[39mlayer_idx)\n\u001b[1;32m--> 732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[1;32mF:\\Doby\\daiv_llm\\model\\llama\\modeling_llama.py:245\u001b[0m, in \u001b[0;36mLlamaMLP.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mintermediate_size\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn \u001b[38;5;241m=\u001b[39m ACT2FN[config\u001b[38;5;241m.\u001b[39mhidden_act]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((out_features, in_features), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 180355072 bytes."
     ]
    }
   ],
   "source": [
    "import llama\n",
    "config = llama.LlamaConfig()\n",
    "model = llama.LlamaModel(config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
