{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e71adb-8e2d-4e83-9bcf-c1fe4d207fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:33:07.151503Z",
     "iopub.status.busy": "2024-03-09T03:33:07.150466Z",
     "iopub.status.idle": "2024-03-09T03:33:07.168669Z",
     "shell.execute_reply": "2024-03-09T03:33:07.167411Z",
     "shell.execute_reply.started": "2024-03-09T03:33:07.151503Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall transformers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f12e791-faa8-408e-94d8-d0c64d6f2668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T04:47:25.471694Z",
     "iopub.status.busy": "2024-03-09T04:47:25.471222Z",
     "iopub.status.idle": "2024-03-09T04:47:25.481168Z",
     "shell.execute_reply": "2024-03-09T04:47:25.479181Z",
     "shell.execute_reply.started": "2024-03-09T04:47:25.471694Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers@cae78c46\n",
      "  Cloning https://github.com/huggingface/transformers (to revision cae78c46) to c:\\users\\sohtk\\appdata\\local\\temp\\pip-req-build-i99cn62k\n",
      "  Resolved https://github.com/huggingface/transformers to commit cae78c46\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from transformers==4.28.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from transformers==4.28.0.dev0) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from transformers==4.28.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from transformers==4.28.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from transformers==4.28.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from transformers==4.28.0.dev0) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from transformers==4.28.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from transformers==4.28.0.dev0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from transformers==4.28.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.28.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sohtk\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0.dev0) (2023.11.17)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml): started\n",
      "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6827676 sha256=e0dec2747e48177bc51f727f004d096bc50a7971c095c9a3303b87e1c5c171da\n",
      "  Stored in directory: C:\\Users\\sohtk\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-m4xukjdo\\wheels\\4a\\c0\\f5\\3f9475f1702539aa2e01fd1e681ad9a4dd8de9f81b39d78acf\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.28.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\sohtk\\AppData\\Local\\Temp\\pip-req-build-i99cn62k'\n",
      "  WARNING: Did not find branch or tag 'cae78c46', assuming revision or ref.\n",
      "  Running command git checkout -q cae78c46\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers@cae78c46\n",
    "# 위 두 커맨드라인으로 Hugging Face 버전 맞춰주도록 할 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959ba09-30b6-4424-980c-25404d062450",
   "metadata": {},
   "source": [
    "# <code>LlamaConfig</code> class 작업\n",
    "* transformers 모듈에서는 해당 관련 스크립트를 제공하지 않는다. (깃허브 오픈소스에는 있음)\n",
    "* 그래서 따로 오픈 소스에서 긁어와서 작업하도록 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54adbe7-0d30-48b2-b9f3-74bb70d03aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:33:57.493793Z",
     "iopub.status.busy": "2024-03-09T03:33:57.492795Z",
     "iopub.status.idle": "2024-03-09T03:33:57.501779Z",
     "shell.execute_reply": "2024-03-09T03:33:57.499861Z",
     "shell.execute_reply.started": "2024-03-09T03:33:57.493793Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama import LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fafe83ce-16ca-462c-aba1-a347e3dab0f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:33:57.974597Z",
     "iopub.status.busy": "2024-03-09T03:33:57.973511Z",
     "iopub.status.idle": "2024-03-09T03:33:57.982481Z",
     "shell.execute_reply": "2024-03-09T03:33:57.981193Z",
     "shell.execute_reply.started": "2024-03-09T03:33:57.974597Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This is the configuration class to store the configuration of a [`LlamaModel`]. It is used to instantiate an LLaMA\n",
      "    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
      "    defaults will yield a similar configuration to that of the LLaMA-7B.\n",
      "\n",
      "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
      "    documentation from [`PretrainedConfig`] for more information.\n",
      "    번역: 해당 클래스는 ['LlamaModel']의 Config를 사용하기 위해서 선언된 클래스다.\n",
      "    즉, LLaMA 모델에 대해서 각 버전에 따라 사용할 수 있도록 해당 클래스를 통해 Configuration을 제공한다.\n",
      "    Arugment에 따라 모델 아키텍처가 정의되고, 그에 따라 사용하려는 모델의 아키텍처의 Config를 받을 수 있으며,\n",
      "    기본적으로는 LLaMA-7B 모델을 제공한다.\n",
      "\n",
      "\n",
      "    Args:\n",
      "        vocab_size (`int`, *optional*, defaults to 32000):\n",
      "            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n",
      "            `inputs_ids` passed when calling [`LlamaModel`]\n",
      "        hidden_size (`int`, *optional*, defaults to 4096):\n",
      "            Dimension of the hidden representations.\n",
      "        intermediate_size (`int`, *optional*, defaults to 11008):\n",
      "            Dimension of the MLP representations.\n",
      "        num_hidden_layers (`int`, *optional*, defaults to 32):\n",
      "            Number of hidden layers in the Transformer decoder.\n",
      "        num_attention_heads (`int`, *optional*, defaults to 32):\n",
      "            Number of attention heads for each attention layer in the Transformer decoder.\n",
      "        num_key_value_heads (`int`, *optional*):\n",
      "            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
      "            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
      "            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n",
      "            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n",
      "            by meanpooling all the original heads within that group. For more details checkout [this\n",
      "            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n",
      "            `num_attention_heads`.\n",
      "        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n",
      "            The non-linear activation function (function or string) in the decoder.\n",
      "        max_position_embeddings (`int`, *optional*, defaults to 2048):\n",
      "            The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n",
      "            Llama 2 up to 4096, CodeLlama up to 16384.\n",
      "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
      "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
      "        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n",
      "            The epsilon used by the rms normalization layers.\n",
      "        use_cache (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
      "            relevant if `config.is_decoder=True`.\n",
      "        pad_token_id (`int`, *optional*):\n",
      "            Padding token id.\n",
      "        bos_token_id (`int`, *optional*, defaults to 1):\n",
      "            Beginning of stream token id.\n",
      "        eos_token_id (`int`, *optional*, defaults to 2):\n",
      "            End of stream token id.\n",
      "        pretraining_tp (`int`, *optional*, defaults to 1):\n",
      "            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
      "            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to understand more about it. This value is\n",
      "            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
      "            issue](https://github.com/pytorch/pytorch/issues/76232).\n",
      "        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to tie weight embeddings\n",
      "        rope_theta (`float`, *optional*, defaults to 10000.0):\n",
      "            The base period of the RoPE embeddings.\n",
      "        rope_scaling (`Dict`, *optional*):\n",
      "            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
      "            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
      "            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
      "            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n",
      "            these scaling strategies behave:\n",
      "            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n",
      "            experimental feature, subject to breaking API changes in future versions.\n",
      "        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n",
      "            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n",
      "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
      "            The dropout ratio for the attention probabilities.\n",
      "\n",
      "    ```python\n",
      "    >>> from transformers import LlamaModel, LlamaConfig\n",
      "\n",
      "    >>> # Initializing a LLaMA llama-7b style configuration\n",
      "    >>> configuration = LlamaConfig()\n",
      "\n",
      "    >>> # Initializing a model from the llama-7b style configuration\n",
      "    >>> model = LlamaModel(configuration)\n",
      "\n",
      "    >>> # Accessing the model configuration\n",
      "    >>> configuration = model.config\n",
      "    ```\n"
     ]
    }
   ],
   "source": [
    "print(LlamaConfig.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20517c0b-4889-4156-a8be-ef64cab43a03",
   "metadata": {},
   "source": [
    "# LLaMA 패키지 만들기 (<code>LlamaModel</code> class 작업)\n",
    "* <code>modeling_llama.py</code>를 사용하기 위해 해당 파일 내에서 import하는 패키지를 전부 from transformers로 수정해주는 작업을 진행 중\n",
    "* 깃허브 오픈소스에는 있으나 pip를 통해 다운받은 라이브러리 내에는 없는 부분들을 발견\n",
    "* 이를 <code>transformers_not_downloaded</code>라는 이름의 패키지를 생성하고, 깃허브 오픈소스에서 없는 파일들을 가져와서 쓸 수 있게끔 수정하였음\n",
    "* * *\n",
    "* 또한, 이 작업이 끝난 뒤에 <code>LlamaConfig</code>를 통해서 model을 선언하려고 했으나\n",
    "* <code>LLamaConfig</code>에서 <code>_attn_implementation</code>이라는 attribute가 없다는 오류가 발생\n",
    "* <code>LLamaConfig가 상속받는 <code>PretrainedConfig</code>에도 해당 attribute는 없었기 떄문에\n",
    "* <code>LLamaConfig</code>에서 <code>_attn_implementation</code>이라는 attribute 선언 후, default로 <code>LlamaAttetion</code>을 수행하게 함.\n",
    "* * *\n",
    "* 이제 준비는 다 되었으나 메모리가 터져서 RuntimeError 발생..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "398b3595-de28-4e40-b51e-00bb2e2fe762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:09.746321Z",
     "iopub.status.busy": "2024-03-09T03:34:09.745709Z",
     "iopub.status.idle": "2024-03-09T03:34:09.752690Z",
     "shell.execute_reply": "2024-03-09T03:34:09.751693Z",
     "shell.execute_reply.started": "2024-03-09T03:34:09.746321Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06253fb6-88bf-4426-8bfb-b5c636be896f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:10.418194Z",
     "iopub.status.busy": "2024-03-09T03:34:10.417251Z",
     "iopub.status.idle": "2024-03-09T03:34:10.432878Z",
     "shell.execute_reply": "2024-03-09T03:34:10.430882Z",
     "shell.execute_reply.started": "2024-03-09T03:34:10.418194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\sohtk\\\\OneDrive\\\\desktop\\\\2024-1\\\\deep_daiv\\\\LLaVA-Med-plus-EEG\\\\Model',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\python311.zip',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\DLLs',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\Lib',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3',\n",
       " '',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\Lib\\\\site-packages',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\Lib\\\\site-packages\\\\win32',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c053bf2-fa60-42a3-92ff-9cd6bdd62c8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:12.798355Z",
     "iopub.status.busy": "2024-03-09T03:34:12.798355Z",
     "iopub.status.idle": "2024-03-09T03:34:12.812312Z",
     "shell.execute_reply": "2024-03-09T03:34:12.810322Z",
     "shell.execute_reply.started": "2024-03-09T03:34:12.798355Z"
    }
   },
   "outputs": [],
   "source": [
    "pck_path = os.path.abspath('.') + '\\\\transformers_not_downloaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d61e55-235c-41aa-b35c-47c845fe9e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:13.827598Z",
     "iopub.status.busy": "2024-03-09T03:34:13.826602Z",
     "iopub.status.idle": "2024-03-09T03:34:13.849540Z",
     "shell.execute_reply": "2024-03-09T03:34:13.847546Z",
     "shell.execute_reply.started": "2024-03-09T03:34:13.827598Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(pck_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae0d201b-17b6-472d-abda-c203d6d8315c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:14.469967Z",
     "iopub.status.busy": "2024-03-09T03:34:14.468956Z",
     "iopub.status.idle": "2024-03-09T03:34:14.492886Z",
     "shell.execute_reply": "2024-03-09T03:34:14.491982Z",
     "shell.execute_reply.started": "2024-03-09T03:34:14.469967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\sohtk\\\\OneDrive\\\\desktop\\\\2024-1\\\\deep_daiv\\\\LLaVA-Med-plus-EEG\\\\Model',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\python311.zip',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\DLLs',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\Lib',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3',\n",
       " '',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\Lib\\\\site-packages',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\Lib\\\\site-packages\\\\win32',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'c:\\\\Users\\\\sohtk\\\\anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin',\n",
       " 'c:\\\\Users\\\\sohtk\\\\OneDrive\\\\desktop\\\\2024-1\\\\deep_daiv\\\\LLaVA-Med-plus-EEG\\\\Model\\\\transformers_not_downloaded']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6043580e-82f8-4f7b-83b2-0e4cc8c27baa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:25.252116Z",
     "iopub.status.busy": "2024-03-09T03:34:25.251124Z",
     "iopub.status.idle": "2024-03-09T03:34:25.271072Z",
     "shell.execute_reply": "2024-03-09T03:34:25.270071Z",
     "shell.execute_reply.started": "2024-03-09T03:34:25.252116Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c99d5587-db96-4c8a-8c56-b9b1130e57c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:28.463489Z",
     "iopub.status.busy": "2024-03-09T03:34:28.463489Z",
     "iopub.status.idle": "2024-03-09T03:34:29.261358Z",
     "shell.execute_reply": "2024-03-09T03:34:29.259362Z",
     "shell.execute_reply.started": "2024-03-09T03:34:28.463489Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.cache_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache, StaticCache\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.cache_utils'"
     ]
    }
   ],
   "source": [
    "## 아직 download 못 받아서 비교차 만든 cell\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b432217-b3f0-469e-a7e5-3ffa6bb00e80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:29.930568Z",
     "iopub.status.busy": "2024-03-09T03:34:29.929570Z",
     "iopub.status.idle": "2024-03-09T03:34:29.950516Z",
     "shell.execute_reply": "2024-03-09T03:34:29.949517Z",
     "shell.execute_reply.started": "2024-03-09T03:34:29.930568Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers_not_downloaded.cache_utils import Cache, DynamicCache, StaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42356d58-b0dd-4de4-ae8a-0d6aeda1bec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:31.283306Z",
     "iopub.status.busy": "2024-03-09T03:34:31.282163Z",
     "iopub.status.idle": "2024-03-09T03:34:31.318068Z",
     "shell.execute_reply": "2024-03-09T03:34:31.315076Z",
     "shell.execute_reply.started": "2024-03-09T03:34:31.283306Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.modeling_attn_mask_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttentionMaskConverter\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling_attn_mask_utils'"
     ]
    }
   ],
   "source": [
    "## 아직 download 못 받아서 비교차 만든 cell\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2114eff-b631-40ab-a8b1-4da15bf0fd04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:33.037584Z",
     "iopub.status.busy": "2024-03-09T03:34:33.037180Z",
     "iopub.status.idle": "2024-03-09T03:34:33.050549Z",
     "shell.execute_reply": "2024-03-09T03:34:33.048555Z",
     "shell.execute_reply.started": "2024-03-09T03:34:33.037584Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers_not_downloaded.modeling_attn_mask_utils import AttentionMaskConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "347201dd-21c1-47ef-a22f-a955ce9ee9d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:33.975376Z",
     "iopub.status.busy": "2024-03-09T03:34:33.974264Z",
     "iopub.status.idle": "2024-03-09T03:34:33.989216Z",
     "shell.execute_reply": "2024-03-09T03:34:33.987213Z",
     "shell.execute_reply.started": "2024-03-09T03:34:33.975376Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "040fb4ba-31bd-48ee-85b2-278911aa72c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:34.323359Z",
     "iopub.status.busy": "2024-03-09T03:34:34.322389Z",
     "iopub.status.idle": "2024-03-09T03:34:34.329361Z",
     "shell.execute_reply": "2024-03-09T03:34:34.328346Z",
     "shell.execute_reply.started": "2024-03-09T03:34:34.323359Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "481df16c-6ce8-4609-8ae1-0308f40005f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:34.828112Z",
     "iopub.status.busy": "2024-03-09T03:34:34.827744Z",
     "iopub.status.idle": "2024-03-09T03:34:34.848688Z",
     "shell.execute_reply": "2024-03-09T03:34:34.847689Z",
     "shell.execute_reply.started": "2024-03-09T03:34:34.828112Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecf5221c-8bc9-4d4d-9341-ef0047fdc6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:35.372299Z",
     "iopub.status.busy": "2024-03-09T03:34:35.371408Z",
     "iopub.status.idle": "2024-03-09T03:34:35.413200Z",
     "shell.execute_reply": "2024-03-09T03:34:35.411210Z",
     "shell.execute_reply.started": "2024-03-09T03:34:35.371408Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_flash_attn_2_available' from 'transformers.utils' (c:\\Users\\sohtk\\anaconda3\\Lib\\site-packages\\transformers\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     add_start_docstrings,\n\u001b[0;32m      3\u001b[0m     add_start_docstrings_to_model_forward,\n\u001b[0;32m      4\u001b[0m     is_flash_attn_2_available,\n\u001b[0;32m      5\u001b[0m     is_flash_attn_greater_or_equal_2_10,\n\u001b[0;32m      6\u001b[0m     logging,\n\u001b[0;32m      7\u001b[0m     replace_return_docstrings,\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_flash_attn_2_available' from 'transformers.utils' (c:\\Users\\sohtk\\anaconda3\\Lib\\site-packages\\transformers\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "## 아직 download 못 받아서 비교차 만든 cell\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db18564c-04f8-4994-85f1-c0b999b385d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T03:34:35.845541Z",
     "iopub.status.busy": "2024-03-09T03:34:35.844515Z",
     "iopub.status.idle": "2024-03-09T03:34:35.853489Z",
     "shell.execute_reply": "2024-03-09T03:34:35.851574Z",
     "shell.execute_reply.started": "2024-03-09T03:34:35.845541Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "\n",
    "from transformers_not_downloaded.utils import (\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71852cde-25bd-4d5c-b595-1062974d6707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T04:20:06.728429Z",
     "iopub.status.busy": "2024-03-09T04:20:06.727905Z",
     "iopub.status.idle": "2024-03-09T04:28:25.616825Z",
     "shell.execute_reply": "2024-03-09T04:28:25.612838Z",
     "shell.execute_reply.started": "2024-03-09T04:20:06.728429Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import llama\n",
    "config = llama.LlamaConfig()\n",
    "model = llama.LlamaModel(config=config)\n",
    "\n",
    "# 다운 laptop에서 메모리 부족으로 안 돌아감 (RAM 16GB)\n",
    "# 서연 laptop에서 돌아감 (RAM 32GB)\n",
    "  # 그럼 30GB 정도의 여유만 있으면 돌아가는건가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8e538e-15d5-47ad-bcb2-dd93a10a2619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:25:19.744261Z",
     "iopub.status.busy": "2024-03-18T13:25:19.743255Z",
     "iopub.status.idle": "2024-03-18T13:25:19.768189Z",
     "shell.execute_reply": "2024-03-18T13:25:19.766192Z",
     "shell.execute_reply.started": "2024-03-18T13:25:19.744261Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_memory_size(model):\n",
    "    '''\n",
    "    param.size() = parameter(Tensor)의 사이즈, e.g. torch.Size([30522, 768])\n",
    "    param.element_size() = Tensor의 element가 가지는 사이즈, 어떤 Type이며, 그 사이즈가 무엇인가\n",
    "    '''\n",
    "    total_params = 0\n",
    "    for param_name, param in model.named_parameters():\n",
    "        total_params += np.prod(param.size()) * param.element_size()\n",
    "    total_size = total_params / (1024 ** 2) # Convert to MB\n",
    "\n",
    "    return f'{total_size:.2f}MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cfb3f-adb4-4004-b0b6-11dab4ca7d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_memory_size(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
