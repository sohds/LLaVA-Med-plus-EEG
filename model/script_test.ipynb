{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583b81f8-0336-4a65-9ed4-2f465194730c",
   "metadata": {},
   "source": [
    "# Hugging Face <code>AutoModelForCausalLM</code> code Review\n",
    "#### [Link](https://medium.com/@cltkzl12/llm-transformers-code-overview-automodelforcausallm-8600ca2a471)\n",
    "* * *\n",
    "* CausalLM인 이유는 Decoder 기반으로서 앞의 토큰만을 이용하여 뒤의 토큰을 예측하기 때문이라고 한다.\n",
    "* <code>AutoModelForCausalLM</code>의 역할은 HuggingFace에서 config, model을 다운받고 불러오는 역할을 하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e71adb-8e2d-4e83-9bcf-c1fe4d207fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T10:50:48.762987Z",
     "iopub.status.busy": "2024-03-08T10:50:48.761992Z",
     "iopub.status.idle": "2024-03-08T10:50:48.778938Z",
     "shell.execute_reply": "2024-03-08T10:50:48.776943Z",
     "shell.execute_reply.started": "2024-03-08T10:50:48.762987Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall transformers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f12e791-faa8-408e-94d8-d0c64d6f2668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T10:50:49.075458Z",
     "iopub.status.busy": "2024-03-08T10:50:49.075458Z",
     "iopub.status.idle": "2024-03-08T10:50:49.088924Z",
     "shell.execute_reply": "2024-03-08T10:50:49.087457Z",
     "shell.execute_reply.started": "2024-03-08T10:50:49.075458Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers@cae78c46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ccf7aec-9b25-43a4-8bf7-1c55523848d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T10:50:52.360887Z",
     "iopub.status.busy": "2024-03-08T10:50:52.359856Z",
     "iopub.status.idle": "2024-03-08T10:50:54.083450Z",
     "shell.execute_reply": "2024-03-08T10:50:54.081513Z",
     "shell.execute_reply.started": "2024-03-08T10:50:52.360887Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe002195-543e-433d-b2f3-ec2b2ee42474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T10:50:56.159905Z",
     "iopub.status.busy": "2024-03-08T10:50:56.158905Z",
     "iopub.status.idle": "2024-03-08T10:51:06.630481Z",
     "shell.execute_reply": "2024-03-08T10:51:06.628493Z",
     "shell.execute_reply.started": "2024-03-08T10:50:56.159905Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2af064934d4c22b213e1057c9a9105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81f34bc-378f-4d95-aea1-c3087b6966b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T10:51:06.633472Z",
     "iopub.status.busy": "2024-03-08T10:51:06.632504Z",
     "iopub.status.idle": "2024-03-08T10:51:06.661399Z",
     "shell.execute_reply": "2024-03-08T10:51:06.659405Z",
     "shell.execute_reply.started": "2024-03-08T10:51:06.633472Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight\n",
      "transformer.wpe.weight\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.attn.bias\n",
      "transformer.h.0.attn.masked_bias\n",
      "transformer.h.0.attn.c_attn.weight\n",
      "transformer.h.0.attn.c_attn.bias\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.attn.bias\n",
      "transformer.h.1.attn.masked_bias\n",
      "transformer.h.1.attn.c_attn.weight\n",
      "transformer.h.1.attn.c_attn.bias\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_1.bias\n",
      "transformer.h.2.attn.bias\n",
      "transformer.h.2.attn.masked_bias\n",
      "transformer.h.2.attn.c_attn.weight\n",
      "transformer.h.2.attn.c_attn.bias\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.ln_2.weight\n",
      "transformer.h.2.ln_2.bias\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_1.bias\n",
      "transformer.h.3.attn.bias\n",
      "transformer.h.3.attn.masked_bias\n",
      "transformer.h.3.attn.c_attn.weight\n",
      "transformer.h.3.attn.c_attn.bias\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.ln_2.weight\n",
      "transformer.h.3.ln_2.bias\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_1.bias\n",
      "transformer.h.4.attn.bias\n",
      "transformer.h.4.attn.masked_bias\n",
      "transformer.h.4.attn.c_attn.weight\n",
      "transformer.h.4.attn.c_attn.bias\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.ln_2.weight\n",
      "transformer.h.4.ln_2.bias\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_1.bias\n",
      "transformer.h.5.attn.bias\n",
      "transformer.h.5.attn.masked_bias\n",
      "transformer.h.5.attn.c_attn.weight\n",
      "transformer.h.5.attn.c_attn.bias\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.ln_2.weight\n",
      "transformer.h.5.ln_2.bias\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "transformer.h.6.ln_1.weight\n",
      "transformer.h.6.ln_1.bias\n",
      "transformer.h.6.attn.bias\n",
      "transformer.h.6.attn.masked_bias\n",
      "transformer.h.6.attn.c_attn.weight\n",
      "transformer.h.6.attn.c_attn.bias\n",
      "transformer.h.6.attn.c_proj.weight\n",
      "transformer.h.6.attn.c_proj.bias\n",
      "transformer.h.6.ln_2.weight\n",
      "transformer.h.6.ln_2.bias\n",
      "transformer.h.6.mlp.c_fc.weight\n",
      "transformer.h.6.mlp.c_fc.bias\n",
      "transformer.h.6.mlp.c_proj.weight\n",
      "transformer.h.6.mlp.c_proj.bias\n",
      "transformer.h.7.ln_1.weight\n",
      "transformer.h.7.ln_1.bias\n",
      "transformer.h.7.attn.bias\n",
      "transformer.h.7.attn.masked_bias\n",
      "transformer.h.7.attn.c_attn.weight\n",
      "transformer.h.7.attn.c_attn.bias\n",
      "transformer.h.7.attn.c_proj.weight\n",
      "transformer.h.7.attn.c_proj.bias\n",
      "transformer.h.7.ln_2.weight\n",
      "transformer.h.7.ln_2.bias\n",
      "transformer.h.7.mlp.c_fc.weight\n",
      "transformer.h.7.mlp.c_fc.bias\n",
      "transformer.h.7.mlp.c_proj.weight\n",
      "transformer.h.7.mlp.c_proj.bias\n",
      "transformer.h.8.ln_1.weight\n",
      "transformer.h.8.ln_1.bias\n",
      "transformer.h.8.attn.bias\n",
      "transformer.h.8.attn.masked_bias\n",
      "transformer.h.8.attn.c_attn.weight\n",
      "transformer.h.8.attn.c_attn.bias\n",
      "transformer.h.8.attn.c_proj.weight\n",
      "transformer.h.8.attn.c_proj.bias\n",
      "transformer.h.8.ln_2.weight\n",
      "transformer.h.8.ln_2.bias\n",
      "transformer.h.8.mlp.c_fc.weight\n",
      "transformer.h.8.mlp.c_fc.bias\n",
      "transformer.h.8.mlp.c_proj.weight\n",
      "transformer.h.8.mlp.c_proj.bias\n",
      "transformer.h.9.ln_1.weight\n",
      "transformer.h.9.ln_1.bias\n",
      "transformer.h.9.attn.bias\n",
      "transformer.h.9.attn.masked_bias\n",
      "transformer.h.9.attn.c_attn.weight\n",
      "transformer.h.9.attn.c_attn.bias\n",
      "transformer.h.9.attn.c_proj.weight\n",
      "transformer.h.9.attn.c_proj.bias\n",
      "transformer.h.9.ln_2.weight\n",
      "transformer.h.9.ln_2.bias\n",
      "transformer.h.9.mlp.c_fc.weight\n",
      "transformer.h.9.mlp.c_fc.bias\n",
      "transformer.h.9.mlp.c_proj.weight\n",
      "transformer.h.9.mlp.c_proj.bias\n",
      "transformer.h.10.ln_1.weight\n",
      "transformer.h.10.ln_1.bias\n",
      "transformer.h.10.attn.bias\n",
      "transformer.h.10.attn.masked_bias\n",
      "transformer.h.10.attn.c_attn.weight\n",
      "transformer.h.10.attn.c_attn.bias\n",
      "transformer.h.10.attn.c_proj.weight\n",
      "transformer.h.10.attn.c_proj.bias\n",
      "transformer.h.10.ln_2.weight\n",
      "transformer.h.10.ln_2.bias\n",
      "transformer.h.10.mlp.c_fc.weight\n",
      "transformer.h.10.mlp.c_fc.bias\n",
      "transformer.h.10.mlp.c_proj.weight\n",
      "transformer.h.10.mlp.c_proj.bias\n",
      "transformer.h.11.ln_1.weight\n",
      "transformer.h.11.ln_1.bias\n",
      "transformer.h.11.attn.bias\n",
      "transformer.h.11.attn.masked_bias\n",
      "transformer.h.11.attn.c_attn.weight\n",
      "transformer.h.11.attn.c_attn.bias\n",
      "transformer.h.11.attn.c_proj.weight\n",
      "transformer.h.11.attn.c_proj.bias\n",
      "transformer.h.11.ln_2.weight\n",
      "transformer.h.11.ln_2.bias\n",
      "transformer.h.11.mlp.c_fc.weight\n",
      "transformer.h.11.mlp.c_fc.bias\n",
      "transformer.h.11.mlp.c_proj.weight\n",
      "transformer.h.11.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.state_dict().items():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f05d5db0-378e-4c24-9e17-56145314325b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T09:51:46.303001Z",
     "iopub.status.busy": "2024-03-07T09:51:46.302005Z",
     "iopub.status.idle": "2024-03-07T09:51:46.330929Z",
     "shell.execute_reply": "2024-03-07T09:51:46.327936Z",
     "shell.execute_reply.started": "2024-03-07T09:51:46.303001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['transformer.h.0.ln_1.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a82e75-f115-4555-bba3-516c4dceb520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T10:51:20.676635Z",
     "iopub.status.busy": "2024-03-08T10:51:20.676635Z",
     "iopub.status.idle": "2024-03-08T10:51:20.699574Z",
     "shell.execute_reply": "2024-03-08T10:51:20.698577Z",
     "shell.execute_reply.started": "2024-03-08T10:51:20.676635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['transformer.h.0.ln_1.weight'].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967e52b5-4af3-422f-b2ab-6972b60517ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T10:51:22.798145Z",
     "iopub.status.busy": "2024-03-08T10:51:22.797149Z",
     "iopub.status.idle": "2024-03-08T10:51:22.815100Z",
     "shell.execute_reply": "2024-03-08T10:51:22.814106Z",
     "shell.execute_reply.started": "2024-03-08T10:51:22.798145Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c54adbe7-0d30-48b2-b9f3-74bb70d03aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T10:51:23.662842Z",
     "iopub.status.busy": "2024-03-08T10:51:23.661846Z",
     "iopub.status.idle": "2024-03-08T10:51:23.686778Z",
     "shell.execute_reply": "2024-03-08T10:51:23.685781Z",
     "shell.execute_reply.started": "2024-03-08T10:51:23.662842Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama import LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fafe83ce-16ca-462c-aba1-a347e3dab0f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T10:51:27.378925Z",
     "iopub.status.busy": "2024-03-08T10:51:27.377927Z",
     "iopub.status.idle": "2024-03-08T10:51:27.386896Z",
     "shell.execute_reply": "2024-03-08T10:51:27.384949Z",
     "shell.execute_reply.started": "2024-03-08T10:51:27.378925Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This is the configuration class to store the configuration of a [`LlamaModel`]. It is used to instantiate an LLaMA\n",
      "    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
      "    defaults will yield a similar configuration to that of the LLaMA-7B.\n",
      "\n",
      "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
      "    documentation from [`PretrainedConfig`] for more information.\n",
      "    번역: 해당 클래스는 ['LlamaModel']의 Config를 사용하기 위해서 선언된 클래스다.\n",
      "    즉, LLaMA 모델에 대해서 각 버전에 따라 사용할 수 있도록 해당 클래스를 통해 Configuration을 제공한다.\n",
      "    Arugment에 따라 모델 아키텍처가 정의되고, 그에 따라 사용하려는 모델의 아키텍처의 Config를 받을 수 있으며,\n",
      "    기본적으로는 LLaMA-7B 모델을 제공한다.\n",
      "\n",
      "\n",
      "    Args:\n",
      "        vocab_size (`int`, *optional*, defaults to 32000):\n",
      "            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n",
      "            `inputs_ids` passed when calling [`LlamaModel`]\n",
      "        hidden_size (`int`, *optional*, defaults to 4096):\n",
      "            Dimension of the hidden representations.\n",
      "        intermediate_size (`int`, *optional*, defaults to 11008):\n",
      "            Dimension of the MLP representations.\n",
      "        num_hidden_layers (`int`, *optional*, defaults to 32):\n",
      "            Number of hidden layers in the Transformer decoder.\n",
      "        num_attention_heads (`int`, *optional*, defaults to 32):\n",
      "            Number of attention heads for each attention layer in the Transformer decoder.\n",
      "        num_key_value_heads (`int`, *optional*):\n",
      "            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
      "            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
      "            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n",
      "            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n",
      "            by meanpooling all the original heads within that group. For more details checkout [this\n",
      "            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n",
      "            `num_attention_heads`.\n",
      "        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n",
      "            The non-linear activation function (function or string) in the decoder.\n",
      "        max_position_embeddings (`int`, *optional*, defaults to 2048):\n",
      "            The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n",
      "            Llama 2 up to 4096, CodeLlama up to 16384.\n",
      "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
      "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
      "        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n",
      "            The epsilon used by the rms normalization layers.\n",
      "        use_cache (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
      "            relevant if `config.is_decoder=True`.\n",
      "        pad_token_id (`int`, *optional*):\n",
      "            Padding token id.\n",
      "        bos_token_id (`int`, *optional*, defaults to 1):\n",
      "            Beginning of stream token id.\n",
      "        eos_token_id (`int`, *optional*, defaults to 2):\n",
      "            End of stream token id.\n",
      "        pretraining_tp (`int`, *optional*, defaults to 1):\n",
      "            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
      "            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to understand more about it. This value is\n",
      "            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
      "            issue](https://github.com/pytorch/pytorch/issues/76232).\n",
      "        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to tie weight embeddings\n",
      "        rope_theta (`float`, *optional*, defaults to 10000.0):\n",
      "            The base period of the RoPE embeddings.\n",
      "        rope_scaling (`Dict`, *optional*):\n",
      "            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
      "            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
      "            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
      "            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n",
      "            these scaling strategies behave:\n",
      "            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n",
      "            experimental feature, subject to breaking API changes in future versions.\n",
      "        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n",
      "            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n",
      "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
      "            The dropout ratio for the attention probabilities.\n",
      "\n",
      "    ```python\n",
      "    >>> from transformers import LlamaModel, LlamaConfig\n",
      "\n",
      "    >>> # Initializing a LLaMA llama-7b style configuration\n",
      "    >>> configuration = LlamaConfig()\n",
      "\n",
      "    >>> # Initializing a model from the llama-7b style configuration\n",
      "    >>> model = LlamaModel(configuration)\n",
      "\n",
      "    >>> # Accessing the model configuration\n",
      "    >>> configuration = model.config\n",
      "    ```\n"
     ]
    }
   ],
   "source": [
    "print(LlamaConfig.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20517c0b-4889-4156-a8be-ef64cab43a03",
   "metadata": {},
   "source": [
    "# LLaMA 패키지 만들기\n",
    "* <code>modeling_llama.py</code>를 사용하기 위해 해당 파일 내에서 import하는 패키지를 전부 from transformers로 수정해주는 작업을 진행 중\n",
    "* 깃허브 오픈소스에는 있으나 pip를 통해 다운받은 라이브러리 내에는 없는 부분들을 발견\n",
    "* 이를 <code>transformers_not_downloaded</code>라는 이름의 패키지를 생성하고, 깃허브 오픈소스에서 없는 파일들을 가져와서 쓸 수 있게끔 수정하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "398b3595-de28-4e40-b51e-00bb2e2fe762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:50:53.850167Z",
     "iopub.status.busy": "2024-03-08T11:50:53.848171Z",
     "iopub.status.idle": "2024-03-08T11:50:53.874104Z",
     "shell.execute_reply": "2024-03-08T11:50:53.872109Z",
     "shell.execute_reply.started": "2024-03-08T11:50:53.850167Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06253fb6-88bf-4426-8bfb-b5c636be896f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:50:53.884078Z",
     "iopub.status.busy": "2024-03-08T11:50:53.883080Z",
     "iopub.status.idle": "2024-03-08T11:50:53.905019Z",
     "shell.execute_reply": "2024-03-08T11:50:53.904023Z",
     "shell.execute_reply.started": "2024-03-08T11:50:53.883080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:\\\\Doby\\\\daiv_llm\\\\model',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\python310.zip',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c053bf2-fa60-42a3-92ff-9cd6bdd62c8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:50:53.908012Z",
     "iopub.status.busy": "2024-03-08T11:50:53.907395Z",
     "iopub.status.idle": "2024-03-08T11:50:53.921995Z",
     "shell.execute_reply": "2024-03-08T11:50:53.918983Z",
     "shell.execute_reply.started": "2024-03-08T11:50:53.908012Z"
    }
   },
   "outputs": [],
   "source": [
    "pck_path = os.path.abspath('.') + '\\\\transformers_not_downloaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d61e55-235c-41aa-b35c-47c845fe9e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:50:53.925964Z",
     "iopub.status.busy": "2024-03-08T11:50:53.925058Z",
     "iopub.status.idle": "2024-03-08T11:50:53.937942Z",
     "shell.execute_reply": "2024-03-08T11:50:53.933943Z",
     "shell.execute_reply.started": "2024-03-08T11:50:53.925964Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(pck_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0d201b-17b6-472d-abda-c203d6d8315c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:50:53.945909Z",
     "iopub.status.busy": "2024-03-08T11:50:53.945909Z",
     "iopub.status.idle": "2024-03-08T11:50:53.966854Z",
     "shell.execute_reply": "2024-03-08T11:50:53.965856Z",
     "shell.execute_reply.started": "2024-03-08T11:50:53.945909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:\\\\Doby\\\\daiv_llm\\\\model',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\python310.zip',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'F:\\\\Doby\\\\daiv_llm\\\\model\\\\transformers_not_downloaded']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6043580e-82f8-4f7b-83b2-0e4cc8c27baa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:50:53.969846Z",
     "iopub.status.busy": "2024-03-08T11:50:53.968870Z",
     "iopub.status.idle": "2024-03-08T11:50:57.914460Z",
     "shell.execute_reply": "2024-03-08T11:50:57.913463Z",
     "shell.execute_reply.started": "2024-03-08T11:50:53.969846Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c99d5587-db96-4c8a-8c56-b9b1130e57c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:50:57.917451Z",
     "iopub.status.busy": "2024-03-08T11:50:57.916453Z",
     "iopub.status.idle": "2024-03-08T11:50:58.788124Z",
     "shell.execute_reply": "2024-03-08T11:50:58.785215Z",
     "shell.execute_reply.started": "2024-03-08T11:50:57.917451Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.cache_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache, StaticCache\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.cache_utils'"
     ]
    }
   ],
   "source": [
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b432217-b3f0-469e-a7e5-3ffa6bb00e80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:51:05.458922Z",
     "iopub.status.busy": "2024-03-08T11:51:05.457862Z",
     "iopub.status.idle": "2024-03-08T11:51:05.476812Z",
     "shell.execute_reply": "2024-03-08T11:51:05.474850Z",
     "shell.execute_reply.started": "2024-03-08T11:51:05.458922Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers_not_downloaded.cache_utils import Cache, DynamicCache, StaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42356d58-b0dd-4de4-ae8a-0d6aeda1bec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:51:05.842610Z",
     "iopub.status.busy": "2024-03-08T11:51:05.842040Z",
     "iopub.status.idle": "2024-03-08T11:51:05.880936Z",
     "shell.execute_reply": "2024-03-08T11:51:05.878940Z",
     "shell.execute_reply.started": "2024-03-08T11:51:05.842610Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.modeling_attn_mask_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttentionMaskConverter\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling_attn_mask_utils'"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2114eff-b631-40ab-a8b1-4da15bf0fd04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:51:06.138270Z",
     "iopub.status.busy": "2024-03-08T11:51:06.136932Z",
     "iopub.status.idle": "2024-03-08T11:51:06.157641Z",
     "shell.execute_reply": "2024-03-08T11:51:06.156695Z",
     "shell.execute_reply.started": "2024-03-08T11:51:06.138270Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers_not_downloaded.modeling_attn_mask_utils import AttentionMaskConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "347201dd-21c1-47ef-a22f-a955ce9ee9d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:51:06.602803Z",
     "iopub.status.busy": "2024-03-08T11:51:06.602803Z",
     "iopub.status.idle": "2024-03-08T11:51:06.658647Z",
     "shell.execute_reply": "2024-03-08T11:51:06.656652Z",
     "shell.execute_reply.started": "2024-03-08T11:51:06.602803Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "040fb4ba-31bd-48ee-85b2-278911aa72c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:51:07.103622Z",
     "iopub.status.busy": "2024-03-08T11:51:07.102624Z",
     "iopub.status.idle": "2024-03-08T11:51:07.808309Z",
     "shell.execute_reply": "2024-03-08T11:51:07.807341Z",
     "shell.execute_reply.started": "2024-03-08T11:51:07.103622Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "481df16c-6ce8-4609-8ae1-0308f40005f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:51:07.812301Z",
     "iopub.status.busy": "2024-03-08T11:51:07.811330Z",
     "iopub.status.idle": "2024-03-08T11:51:07.825269Z",
     "shell.execute_reply": "2024-03-08T11:51:07.823280Z",
     "shell.execute_reply.started": "2024-03-08T11:51:07.812301Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecf5221c-8bc9-4d4d-9341-ef0047fdc6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:51:08.379296Z",
     "iopub.status.busy": "2024-03-08T11:51:08.378785Z",
     "iopub.status.idle": "2024-03-08T11:51:08.419676Z",
     "shell.execute_reply": "2024-03-08T11:51:08.415688Z",
     "shell.execute_reply.started": "2024-03-08T11:51:08.379296Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_flash_attn_2_available' from 'transformers.utils' (C:\\Users\\user\\anaconda3\\lib\\site-packages\\transformers\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     add_start_docstrings,\n\u001b[0;32m      3\u001b[0m     add_start_docstrings_to_model_forward,\n\u001b[0;32m      4\u001b[0m     is_flash_attn_2_available,\n\u001b[0;32m      5\u001b[0m     is_flash_attn_greater_or_equal_2_10,\n\u001b[0;32m      6\u001b[0m     logging,\n\u001b[0;32m      7\u001b[0m     replace_return_docstrings,\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_flash_attn_2_available' from 'transformers.utils' (C:\\Users\\user\\anaconda3\\lib\\site-packages\\transformers\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db18564c-04f8-4994-85f1-c0b999b385d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:51:09.460893Z",
     "iopub.status.busy": "2024-03-08T11:51:09.460302Z",
     "iopub.status.idle": "2024-03-08T11:51:09.618471Z",
     "shell.execute_reply": "2024-03-08T11:51:09.617501Z",
     "shell.execute_reply.started": "2024-03-08T11:51:09.460893Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "\n",
    "from transformers_not_downloaded.utils import (\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71852cde-25bd-4d5c-b595-1062974d6707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T11:59:10.844914Z",
     "iopub.status.busy": "2024-03-08T11:59:10.843918Z",
     "iopub.status.idle": "2024-03-08T11:59:12.597336Z",
     "shell.execute_reply": "2024-03-08T11:59:12.596405Z",
     "shell.execute_reply.started": "2024-03-08T11:59:10.844914Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaConfig' object has no attribute '_attn_implementation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mllama\u001b[39;00m\n\u001b[0;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m llama\u001b[38;5;241m.\u001b[39mLlamaConfig()\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\Doby\\daiv_llm\\model\\llama\\modeling_llama.py:955\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m--> 955\u001b[0m     [LlamaDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[0;32m    956\u001b[0m )\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Doby\\daiv_llm\\model\\llama\\modeling_llama.py:955\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m--> 955\u001b[0m     [\u001b[43mLlamaDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[0;32m    956\u001b[0m )\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Doby\\daiv_llm\\model\\llama\\modeling_llama.py:726\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[1;34m(self, config, layer_idx)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m LLAMA_ATTENTION_CLASSES[\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn_implementation\u001b[49m](config\u001b[38;5;241m=\u001b[39mconfig, layer_idx\u001b[38;5;241m=\u001b[39mlayer_idx)\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m LlamaMLP(config)\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py:260\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    259\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[1;32m--> 260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LlamaConfig' object has no attribute '_attn_implementation'"
     ]
    }
   ],
   "source": [
    "# 이 부분 오류부터 해결하면 된다.\n",
    "import llama\n",
    "config = llama.LlamaConfig()\n",
    "model = llama.LlamaModel(config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
