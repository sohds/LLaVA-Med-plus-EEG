{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e71adb-8e2d-4e83-9bcf-c1fe4d207fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:11:52.586701Z",
     "iopub.status.busy": "2024-03-18T14:11:52.584702Z",
     "iopub.status.idle": "2024-03-18T14:11:52.610632Z",
     "shell.execute_reply": "2024-03-18T14:11:52.608715Z",
     "shell.execute_reply.started": "2024-03-18T14:11:52.585696Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall transformers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f12e791-faa8-408e-94d8-d0c64d6f2668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:11:52.614616Z",
     "iopub.status.busy": "2024-03-18T14:11:52.614616Z",
     "iopub.status.idle": "2024-03-18T14:11:52.626584Z",
     "shell.execute_reply": "2024-03-18T14:11:52.624590Z",
     "shell.execute_reply.started": "2024-03-18T14:11:52.614616Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers@cae78c46\n",
    "# 위 두 커맨드라인으로 Hugging Face 버전 맞춰주도록 할 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959ba09-30b6-4424-980c-25404d062450",
   "metadata": {},
   "source": [
    "# <code>LlamaConfig</code> class 작업\n",
    "* transformers 모듈에서는 해당 관련 스크립트를 제공하지 않는다. (깃허브 오픈소스에는 있음)\n",
    "* 그래서 따로 오픈 소스에서 긁어와서 작업하도록 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54adbe7-0d30-48b2-b9f3-74bb70d03aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:11:52.630582Z",
     "iopub.status.busy": "2024-03-18T14:11:52.629585Z",
     "iopub.status.idle": "2024-03-18T14:12:01.802073Z",
     "shell.execute_reply": "2024-03-18T14:12:01.800057Z",
     "shell.execute_reply.started": "2024-03-18T14:11:52.630582Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama import LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fafe83ce-16ca-462c-aba1-a347e3dab0f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:01.807036Z",
     "iopub.status.busy": "2024-03-18T14:12:01.806040Z",
     "iopub.status.idle": "2024-03-18T14:12:01.831971Z",
     "shell.execute_reply": "2024-03-18T14:12:01.830972Z",
     "shell.execute_reply.started": "2024-03-18T14:12:01.807036Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This is the configuration class to store the configuration of a [`LlamaModel`]. It is used to instantiate an LLaMA\n",
      "    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
      "    defaults will yield a similar configuration to that of the LLaMA-7B.\n",
      "\n",
      "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
      "    documentation from [`PretrainedConfig`] for more information.\n",
      "    번역: 해당 클래스는 ['LlamaModel']의 Config를 사용하기 위해서 선언된 클래스다.\n",
      "    즉, LLaMA 모델에 대해서 각 버전에 따라 사용할 수 있도록 해당 클래스를 통해 Configuration을 제공한다.\n",
      "    Arugment에 따라 모델 아키텍처가 정의되고, 그에 따라 사용하려는 모델의 아키텍처의 Config를 받을 수 있으며,\n",
      "    기본적으로는 LLaMA-7B 모델을 제공한다.\n",
      "\n",
      "\n",
      "    Args:\n",
      "        vocab_size (`int`, *optional*, defaults to 32000):\n",
      "            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n",
      "            `inputs_ids` passed when calling [`LlamaModel`]\n",
      "        hidden_size (`int`, *optional*, defaults to 4096):\n",
      "            Dimension of the hidden representations.\n",
      "        intermediate_size (`int`, *optional*, defaults to 11008):\n",
      "            Dimension of the MLP representations.\n",
      "        num_hidden_layers (`int`, *optional*, defaults to 32):\n",
      "            Number of hidden layers in the Transformer decoder.\n",
      "        num_attention_heads (`int`, *optional*, defaults to 32):\n",
      "            Number of attention heads for each attention layer in the Transformer decoder.\n",
      "        num_key_value_heads (`int`, *optional*):\n",
      "            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
      "            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
      "            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n",
      "            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n",
      "            by meanpooling all the original heads within that group. For more details checkout [this\n",
      "            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n",
      "            `num_attention_heads`.\n",
      "        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n",
      "            The non-linear activation function (function or string) in the decoder.\n",
      "        max_position_embeddings (`int`, *optional*, defaults to 2048):\n",
      "            The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n",
      "            Llama 2 up to 4096, CodeLlama up to 16384.\n",
      "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
      "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
      "        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n",
      "            The epsilon used by the rms normalization layers.\n",
      "        use_cache (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
      "            relevant if `config.is_decoder=True`.\n",
      "        pad_token_id (`int`, *optional*):\n",
      "            Padding token id.\n",
      "        bos_token_id (`int`, *optional*, defaults to 1):\n",
      "            Beginning of stream token id.\n",
      "        eos_token_id (`int`, *optional*, defaults to 2):\n",
      "            End of stream token id.\n",
      "        pretraining_tp (`int`, *optional*, defaults to 1):\n",
      "            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
      "            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to understand more about it. This value is\n",
      "            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
      "            issue](https://github.com/pytorch/pytorch/issues/76232).\n",
      "        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to tie weight embeddings\n",
      "        rope_theta (`float`, *optional*, defaults to 10000.0):\n",
      "            The base period of the RoPE embeddings.\n",
      "        rope_scaling (`Dict`, *optional*):\n",
      "            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
      "            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
      "            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
      "            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n",
      "            these scaling strategies behave:\n",
      "            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n",
      "            experimental feature, subject to breaking API changes in future versions.\n",
      "        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n",
      "            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n",
      "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
      "            The dropout ratio for the attention probabilities.\n",
      "\n",
      "    ```python\n",
      "    >>> from transformers import LlamaModel, LlamaConfig\n",
      "\n",
      "    >>> # Initializing a LLaMA llama-7b style configuration\n",
      "    >>> configuration = LlamaConfig()\n",
      "\n",
      "    >>> # Initializing a model from the llama-7b style configuration\n",
      "    >>> model = LlamaModel(configuration)\n",
      "\n",
      "    >>> # Accessing the model configuration\n",
      "    >>> configuration = model.config\n",
      "    ```\n"
     ]
    }
   ],
   "source": [
    "print(LlamaConfig.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20517c0b-4889-4156-a8be-ef64cab43a03",
   "metadata": {},
   "source": [
    "# LLaMA 패키지 만들기 (<code>LlamaModel</code> class 작업)\n",
    "* <code>modeling_llama.py</code>를 사용하기 위해 해당 파일 내에서 import하는 패키지를 전부 from transformers로 수정해주는 작업을 진행 중\n",
    "* 깃허브 오픈소스에는 있으나 pip를 통해 다운받은 라이브러리 내에는 없는 부분들을 발견\n",
    "* 이를 <code>transformers_not_downloaded</code>라는 이름의 패키지를 생성하고, 깃허브 오픈소스에서 없는 파일들을 가져와서 쓸 수 있게끔 수정하였음\n",
    "* * *\n",
    "* 또한, 이 작업이 끝난 뒤에 <code>LlamaConfig</code>를 통해서 model을 선언하려고 했으나\n",
    "* <code>LLamaConfig</code>에서 <code>_attn_implementation</code>이라는 attribute가 없다는 오류가 발생\n",
    "* <code>LLamaConfig가 상속받는 <code>PretrainedConfig</code>에도 해당 attribute는 없었기 떄문에\n",
    "* <code>LLamaConfig</code>에서 <code>_attn_implementation</code>이라는 attribute 선언 후, default로 <code>LlamaAttetion</code>을 수행하게 함.\n",
    "* * *\n",
    "* 이제 준비는 다 되었으나 메모리가 터져서 RuntimeError 발생..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "398b3595-de28-4e40-b51e-00bb2e2fe762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:01.834962Z",
     "iopub.status.busy": "2024-03-18T14:12:01.833964Z",
     "iopub.status.idle": "2024-03-18T14:12:01.848933Z",
     "shell.execute_reply": "2024-03-18T14:12:01.846938Z",
     "shell.execute_reply.started": "2024-03-18T14:12:01.834962Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06253fb6-88bf-4426-8bfb-b5c636be896f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:01.850919Z",
     "iopub.status.busy": "2024-03-18T14:12:01.849923Z",
     "iopub.status.idle": "2024-03-18T14:12:01.879843Z",
     "shell.execute_reply": "2024-03-18T14:12:01.878845Z",
     "shell.execute_reply.started": "2024-03-18T14:12:01.850919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:\\\\Doby\\\\daiv_llm\\\\model',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\python310.zip',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c053bf2-fa60-42a3-92ff-9cd6bdd62c8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:01.882842Z",
     "iopub.status.busy": "2024-03-18T14:12:01.881844Z",
     "iopub.status.idle": "2024-03-18T14:12:01.895800Z",
     "shell.execute_reply": "2024-03-18T14:12:01.894802Z",
     "shell.execute_reply.started": "2024-03-18T14:12:01.882842Z"
    }
   },
   "outputs": [],
   "source": [
    "pck_path = os.path.abspath('.') + '\\\\transformers_not_downloaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d61e55-235c-41aa-b35c-47c845fe9e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:01.899800Z",
     "iopub.status.busy": "2024-03-18T14:12:01.898839Z",
     "iopub.status.idle": "2024-03-18T14:12:01.913752Z",
     "shell.execute_reply": "2024-03-18T14:12:01.910767Z",
     "shell.execute_reply.started": "2024-03-18T14:12:01.899800Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(pck_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae0d201b-17b6-472d-abda-c203d6d8315c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:01.915754Z",
     "iopub.status.busy": "2024-03-18T14:12:01.914757Z",
     "iopub.status.idle": "2024-03-18T14:12:01.928712Z",
     "shell.execute_reply": "2024-03-18T14:12:01.926718Z",
     "shell.execute_reply.started": "2024-03-18T14:12:01.915754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:\\\\Doby\\\\daiv_llm\\\\model',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\python310.zip',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'F:\\\\Doby\\\\daiv_llm\\\\model\\\\transformers_not_downloaded']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6043580e-82f8-4f7b-83b2-0e4cc8c27baa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:01.936692Z",
     "iopub.status.busy": "2024-03-18T14:12:01.934714Z",
     "iopub.status.idle": "2024-03-18T14:12:01.960628Z",
     "shell.execute_reply": "2024-03-18T14:12:01.958633Z",
     "shell.execute_reply.started": "2024-03-18T14:12:01.935692Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c99d5587-db96-4c8a-8c56-b9b1130e57c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:01.963700Z",
     "iopub.status.busy": "2024-03-18T14:12:01.962623Z",
     "iopub.status.idle": "2024-03-18T14:12:02.898129Z",
     "shell.execute_reply": "2024-03-18T14:12:02.896134Z",
     "shell.execute_reply.started": "2024-03-18T14:12:01.963700Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.cache_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## 아직 download 못 받아서 비교차 만든 cell\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache, StaticCache\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.cache_utils'"
     ]
    }
   ],
   "source": [
    "## 아직 download 못 받아서 비교차 만든 cell\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b432217-b3f0-469e-a7e5-3ffa6bb00e80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:07.544703Z",
     "iopub.status.busy": "2024-03-18T14:12:07.543707Z",
     "iopub.status.idle": "2024-03-18T14:12:07.555011Z",
     "shell.execute_reply": "2024-03-18T14:12:07.553672Z",
     "shell.execute_reply.started": "2024-03-18T14:12:07.544703Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers_not_downloaded.cache_utils import Cache, DynamicCache, StaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42356d58-b0dd-4de4-ae8a-0d6aeda1bec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:08.098215Z",
     "iopub.status.busy": "2024-03-18T14:12:08.097218Z",
     "iopub.status.idle": "2024-03-18T14:12:08.137111Z",
     "shell.execute_reply": "2024-03-18T14:12:08.135115Z",
     "shell.execute_reply.started": "2024-03-18T14:12:08.098215Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.modeling_attn_mask_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## 아직 download 못 받아서 비교차 만든 cell\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttentionMaskConverter\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling_attn_mask_utils'"
     ]
    }
   ],
   "source": [
    "## 아직 download 못 받아서 비교차 만든 cell\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2114eff-b631-40ab-a8b1-4da15bf0fd04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:08.650500Z",
     "iopub.status.busy": "2024-03-18T14:12:08.649484Z",
     "iopub.status.idle": "2024-03-18T14:12:08.657466Z",
     "shell.execute_reply": "2024-03-18T14:12:08.656465Z",
     "shell.execute_reply.started": "2024-03-18T14:12:08.650500Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers_not_downloaded.modeling_attn_mask_utils import AttentionMaskConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "347201dd-21c1-47ef-a22f-a955ce9ee9d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:09.519714Z",
     "iopub.status.busy": "2024-03-18T14:12:09.518691Z",
     "iopub.status.idle": "2024-03-18T14:12:09.535653Z",
     "shell.execute_reply": "2024-03-18T14:12:09.533661Z",
     "shell.execute_reply.started": "2024-03-18T14:12:09.519714Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "040fb4ba-31bd-48ee-85b2-278911aa72c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:09.881720Z",
     "iopub.status.busy": "2024-03-18T14:12:09.881720Z",
     "iopub.status.idle": "2024-03-18T14:12:09.897686Z",
     "shell.execute_reply": "2024-03-18T14:12:09.895688Z",
     "shell.execute_reply.started": "2024-03-18T14:12:09.881720Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "481df16c-6ce8-4609-8ae1-0308f40005f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:10.244786Z",
     "iopub.status.busy": "2024-03-18T14:12:10.243788Z",
     "iopub.status.idle": "2024-03-18T14:12:10.267817Z",
     "shell.execute_reply": "2024-03-18T14:12:10.265730Z",
     "shell.execute_reply.started": "2024-03-18T14:12:10.244786Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecf5221c-8bc9-4d4d-9341-ef0047fdc6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:10.737469Z",
     "iopub.status.busy": "2024-03-18T14:12:10.737469Z",
     "iopub.status.idle": "2024-03-18T14:12:10.785347Z",
     "shell.execute_reply": "2024-03-18T14:12:10.782356Z",
     "shell.execute_reply.started": "2024-03-18T14:12:10.737469Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_flash_attn_2_available' from 'transformers.utils' (C:\\Users\\user\\anaconda3\\lib\\site-packages\\transformers\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## 아직 download 못 받아서 비교차 만든 cell\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     add_start_docstrings,\n\u001b[0;32m      4\u001b[0m     add_start_docstrings_to_model_forward,\n\u001b[0;32m      5\u001b[0m     is_flash_attn_2_available,\n\u001b[0;32m      6\u001b[0m     is_flash_attn_greater_or_equal_2_10,\n\u001b[0;32m      7\u001b[0m     logging,\n\u001b[0;32m      8\u001b[0m     replace_return_docstrings,\n\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_flash_attn_2_available' from 'transformers.utils' (C:\\Users\\user\\anaconda3\\lib\\site-packages\\transformers\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "## 아직 download 못 받아서 비교차 만든 cell\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db18564c-04f8-4994-85f1-c0b999b385d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:11.151384Z",
     "iopub.status.busy": "2024-03-18T14:12:11.151384Z",
     "iopub.status.idle": "2024-03-18T14:12:11.162376Z",
     "shell.execute_reply": "2024-03-18T14:12:11.160344Z",
     "shell.execute_reply.started": "2024-03-18T14:12:11.151384Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "\n",
    "from transformers_not_downloaded.utils import (\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71852cde-25bd-4d5c-b595-1062974d6707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T14:12:11.884407Z",
     "iopub.status.busy": "2024-03-18T14:12:11.884407Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import llama\n",
    "config = llama.LlamaConfig()\n",
    "model = llama.LlamaModel(config=config)\n",
    "\n",
    "# 다운 laptop에서 메모리 부족으로 안 돌아감 (RAM 16GB)\n",
    "# 서연 laptop에서 돌아감 (RAM 32GB)\n",
    "  # 그럼 30GB 정도의 여유만 있으면 돌아가는건가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e538e-15d5-47ad-bcb2-dd93a10a2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_memory_size(model):\n",
    "    '''\n",
    "    param.size() = parameter(Tensor)의 사이즈, e.g. torch.Size([30522, 768])\n",
    "    param.element_size() = Tensor의 element가 가지는 사이즈, 어떤 Type이며, 그 사이즈가 무엇인가\n",
    "    '''\n",
    "    total_params = 0\n",
    "    for param_name, param in model.named_parameters():\n",
    "        total_params += np.prod(param.size()) * param.element_size()\n",
    "    total_size = total_params / (1024 ** 2) # Convert to MB\n",
    "\n",
    "    return f'{total_size:.2f}MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cfb3f-adb4-4004-b0b6-11dab4ca7d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_memory_size(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
